{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54616499-4aa6-4106-b05f-5b2f8e508329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "root_path = './'  ## replace this with your root path (i.e., path of this current project)\n",
    "os.environ['PYTHONPATH'] = root_path\n",
    "sys.path.append(root_path)\n",
    "import pickle\n",
    "import random\n",
    "import subprocess\n",
    "import torch.cuda\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.earlystopping.protocols import EarlyStopping\n",
    "from utils.test_dataloder import *\n",
    "import datetime\n",
    "from utils.get_time import get_time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from utils.warmup import *\n",
    "import torch.nn.functional as F\n",
    "from models.bartmodel_octuple import Bart\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e696daa-1c57-4b35-8aad-3c65bde8fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b52e9a3-3226-4ca6-982b-c357a3e23789",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_name = ''  ## replace with your checkpoint name\n",
    "ckpt_dir = f'./checkpoints/{ckpt_name}' ## replace with your checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da90aa88-dabb-44e9-8df1-6215d474e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_keys = ['strength', 'length', 'phrase']\n",
    "tgt_keys = ['bar', 'pos', 'token', 'dur', 'phrase']\n",
    "\n",
    "\n",
    "binary_dir = './binary' ## replace with your path to dictionaries\n",
    "words_dir = './binary/words' ## replace with your path to binary words\n",
    "hparams = {\n",
    "    'batch_size': 1,\n",
    "    'word_data_dir': words_dir,\n",
    "    'sentence_maxlen': 512,\n",
    "    'hidden_size': 768,\n",
    "    'n_layers': 6,\n",
    "    'n_head': 8,\n",
    "    'pretrain': '',\n",
    "    'lr': 5.0e-5,\n",
    "    'optimizer_adam_beta1': 0.9,\n",
    "    'optimizer_adam_beta2': 0.98,\n",
    "    'weight_decay': 0.001,\n",
    "    'patience': 5,\n",
    "    'warmup': 2500,\n",
    "    'lr': 5.0e-5,\n",
    "    'checkpoint_dir': './checkpoints', ## replace with your checkpoint directory\n",
    "    'drop_prob': 0.2,\n",
    "    'total_epoch': 1000,\n",
    "    'infer_batch_size': 1,\n",
    "    'temperature': 1.6,\n",
    "    'topk': 5,\n",
    "    'prompt_step': 1,\n",
    "    'infer_max_step': 1024,\n",
    "    'output_dir': \"/home/qihao/CS6207/octuple/output\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f197dd9-a209-46c7-96e8-664a52ed6176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=1234):  # seed setting\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601fb7c-9c8f-4808-8180-c3ba34201b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path, device):\n",
    "    model = Bart(event2word_dict=event2word_dict, \n",
    "                 word2event_dict=word2event_dict, \n",
    "                 model_pth='',\n",
    "                 hidden_size=hparams['hidden_size'], \n",
    "                 num_layers=hparams['n_layers'], \n",
    "                 num_heads=hparams['n_head'], \n",
    "                 dropout=hparams['drop_prob'],).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=True)\n",
    "    model.eval()\n",
    "    print(f\"| Successfully loaded bart ckpt from {checkpoint_path}.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05e2b0-a7ec-4d65-96be-87178a053308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xe_loss(outputs, targets):\n",
    "    outputs = outputs.transpose(1, 2)\n",
    "    return F.cross_entropy(outputs, targets, ignore_index=0, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b780a0-a2b8-4ae8-b51d-d8d3bd5d391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_l2m():\n",
    "    ##     Bart Model\n",
    "    set_seed()\n",
    "    print(f\"Using device: {device} for inferences custom samples\")\n",
    "    \n",
    "    # training conditions (for naming the ckpt)\n",
    "    lr = hparams['lr']\n",
    "    \n",
    "    ckpt_path = os.path.join(ckpt_dir, 'best.pt')\n",
    "\n",
    "    # load dictionary\n",
    "    event2word_dict, word2event_dict = pickle.load(open(f\"{binary_dir}/music_dict.pkl\", 'rb'))\n",
    "    \n",
    "    test_dataset = L2MDataset('test', event2word_dict, hparams, shuffle=True)\n",
    "    test_loader = build_dataloader(dataset=test_dataset, shuffle=True, batch_size=hparams['infer_batch_size'], endless=False)\n",
    "    \n",
    "    print(f\"Test Datalodaer = {len(test_loader)} Songs\")\n",
    "\n",
    "    # load melody generation model based on skeleton framework\n",
    "    model = Bart(event2word_dict=event2word_dict, \n",
    "                 word2event_dict=word2event_dict, \n",
    "                 model_pth='',\n",
    "                 hidden_size=hparams['hidden_size'], \n",
    "                 num_layers=hparams['n_layers'], \n",
    "                 num_heads=hparams['n_head'], \n",
    "                 dropout=hparams['drop_prob'],).to(device)\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device), strict=True)\n",
    "    model.eval()\n",
    "    print(f\"| Successfully loaded bart ckpt from {ckpt_path}.\")\n",
    "\n",
    "    # Inference file path\n",
    "    exp_date = get_time()\n",
    "    melody_output_dir = os.path.join(hparams['output_dir'], f'melody_{exp_date}')\n",
    "    if not os.path.exists(melody_output_dir):\n",
    "        os.mkdir(melody_output_dir)\n",
    "    \n",
    "    ### randomly sample from test data\n",
    "    import random\n",
    "\n",
    "    num_sample = 50\n",
    "    random_integers = random.sample(range(0, len(test_loader)-1), num_sample)\n",
    "    \n",
    "    # prompt_step = hparams['prompt_step']\n",
    "    \n",
    "    for data_idx, data in enumerate(test_loader):\n",
    "        try:\n",
    "            data_name = data['item_name'][0] if '.mid' not in data['item_name'][0] else data['item_name'][0][:-4]\n",
    "\n",
    "            enc_inputs = {k: data[f'src_{k}'].to(device) for k in src_keys}\n",
    "            dec_inputs = {k: data[f'tgt_{k}'].to(device) for k in tgt_keys}\n",
    "            \n",
    "            prompt_step = hparams['prompt_step']\n",
    "            \n",
    "            dec_inputs_selected = {\n",
    "                'bar': dec_inputs['bar'][:, :prompt_step],\n",
    "                'pos': dec_inputs['pos'][:, :prompt_step],\n",
    "                'token': dec_inputs['token'][:, :prompt_step],\n",
    "                'dur': dec_inputs['dur'][:, :prompt_step],\n",
    "                'phrase': dec_inputs['phrase'][:, :prompt_step],\n",
    "            }\n",
    "            \n",
    "            decode_length = dec_inputs['token'].shape[-1]\n",
    "            max_sent_len = 1024\n",
    "            \n",
    "            print(f\"Expected decode length: {decode_length}\")\n",
    "            _ = model.infer(enc_inputs=enc_inputs, \n",
    "                            dec_inputs_gt=dec_inputs_selected, \n",
    "                            decode_length=decode_length,\n",
    "                            sentence_maxlen=max_sent_len, \n",
    "                            temperature=hparams['temperature'], \n",
    "                            topk=hparams['topk'], \n",
    "                            device=device, \n",
    "                            output_dir=melody_output_dir, \n",
    "                            midi_name=data_name)\n",
    "            \n",
    "            print(f\"Generating {data_idx+1}/{len(test_loader)}, Name: {data_name}\")\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-\\nBad Item: {data_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca57e4-ea82-46a9-93b3-85c50395b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_l2m()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xailyr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
