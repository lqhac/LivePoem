{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54616499-4aa6-4106-b05f-5b2f8e508329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import subprocess\n",
    "import torch.cuda\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.earlystopping.protocols import EarlyStopping\n",
    "from test_dataloder import *\n",
    "import datetime\n",
    "from utils.get_time import get_time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from utils.warmup import *\n",
    "import torch.nn.functional as F\n",
    "from bartmodel import Bart\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da90aa88-dabb-44e9-8df1-6215d474e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "src_keys = ['strength', 'length', 'phrase']\n",
    "tgt_keys = ['bar', 'pos', 'token', 'dur', 'phrase']\n",
    "\n",
    "binary_dir = './binary' ## replace with your path to dictionaries\n",
    "words_dir = './binary/words' ## replace with your path to binary words\n",
    "hparams = {\n",
    "    'batch_size': 8,\n",
    "    'word_data_dir': './binary/words', ## replace with your binary words directory\n",
    "    'sentence_maxlen': 512,\n",
    "    'hidden_size': 768,\n",
    "    'n_layers': 6,\n",
    "    'n_head': 8,\n",
    "    'pretrain': '',\n",
    "    'lr': 1.0e-5,\n",
    "    'optimizer_adam_beta1': 0.9,\n",
    "    'optimizer_adam_beta2': 0.98,\n",
    "    'weight_decay': 0.01,\n",
    "    'patience': 5,\n",
    "    'warmup': 3000,\n",
    "    'lr': 5.0e-5,\n",
    "    'checkpoint_dir': './checkpoints',  ## replace with your checkpoint directory\n",
    "    'drop_prob': 0.2,\n",
    "    'total_epoch': 1000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f197dd9-a209-46c7-96e8-664a52ed6176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=1234):  # seed setting\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05e2b0-a7ec-4d65-96be-87178a053308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xe_loss(outputs, targets):\n",
    "    outputs = outputs.transpose(1, 2)\n",
    "    return F.cross_entropy(outputs, targets, ignore_index=0, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fc38f-2432-4ab4-a717-d162972ede5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, scheduler, epoch, total_epoch):\n",
    "    # define the format of tqdm\n",
    "    with tqdm(total=len(train_loader), ncols=150, position=0, leave=True) as _tqdm:  # 总长度是data的长度\n",
    "        _tqdm.set_description('training epoch: {}/{}'.format(epoch + 1, total_epoch))  # 设置前缀更新信息\n",
    "\n",
    "        # Model Train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_loss = []\n",
    "        train_bar_loss = []\n",
    "        train_pos_loss = []\n",
    "        train_token_loss = []\n",
    "        train_dur_loss = []\n",
    "        train_phrase_loss = []\n",
    "\n",
    "        for idx, data in enumerate(train_loader):\n",
    "            # prompt_index = list(data[f'tgt_word'].numpy()).index(50268)\n",
    "            enc_inputs = {k: data[f'src_{k}'].to(device) for k in src_keys}\n",
    "            dec_inputs = {k: data[f'tgt_{k}'].to(device) for k in tgt_keys}\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            bar_out, pos_out, token_out, dur_out, phrase_out = model(enc_inputs, dec_inputs)\n",
    "            \n",
    "            # print(bar_out, bar_out.logit())\n",
    "            \n",
    "            bar_out = bar_out #.logit()\n",
    "            tgt_bar = (data['tgt_bar'].to(device))[:, 1:]\n",
    "            bar_loss = xe_loss(bar_out[:, :-1], tgt_bar)\n",
    "            \n",
    "            pos_out = pos_out #.logit()\n",
    "            tgt_pos = (data['tgt_pos'].to(device))[:, 1:]\n",
    "            pos_loss = xe_loss(pos_out[:, :-1], tgt_pos)\n",
    "            \n",
    "            token_out = token_out #.logit()\n",
    "            tgt_token = (data['tgt_token'].to(device))[:, 1:]\n",
    "            token_loss = xe_loss(token_out[:, :-1], tgt_token)\n",
    "            \n",
    "            dur_out = dur_out #.logit()\n",
    "            tgt_dur = (data['tgt_dur'].to(device))[:, 1:]\n",
    "            dur_loss = xe_loss(dur_out[:, :-1], tgt_dur)\n",
    "            \n",
    "            phrase_out = phrase_out #.logit()\n",
    "            tgt_phrase = (data['tgt_phrase'].to(device))[:, 1:]\n",
    "            phrase_loss = xe_loss(phrase_out[:, :-1], tgt_phrase)\n",
    "            \n",
    "\n",
    "            # 3) total loss\n",
    "            total_loss = bar_loss + pos_loss + token_loss + dur_loss + phrase_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss.append(total_loss.item())\n",
    "            running_loss += total_loss.item()\n",
    "            \n",
    "            train_bar_loss.append(bar_loss.item())\n",
    "            train_pos_loss.append(pos_loss.item())\n",
    "            train_token_loss.append(token_loss.item())\n",
    "            train_dur_loss.append(dur_loss.item())\n",
    "            train_phrase_loss.append(phrase_loss.item())\n",
    "\n",
    "            _tqdm.set_postfix(\n",
    "                loss=\"{:.3f}, bar={:.3f}, pos={:.3f}, token={:.3f}, dur={:.3f}, phrase={:.3f}\".format(total_loss,\n",
    "                                                                                                      bar_loss, \n",
    "                                                                                                      pos_loss,\n",
    "                                                                                                      token_loss,\n",
    "                                                                                                      dur_loss,\n",
    "                                                                                                      phrase_loss))\n",
    "            \n",
    "            _tqdm.update(2)\n",
    "\n",
    "    train_loss_avg = np.mean(train_loss)\n",
    "    train_bar_loss_avg = np.mean(train_bar_loss)\n",
    "    train_pos_loss_avg = np.mean(train_pos_loss)\n",
    "    train_token_loss_avg = np.mean(train_token_loss)\n",
    "    train_dur_loss_avg = np.mean(train_dur_loss)\n",
    "    train_phrase_loss_avg = np.mean(train_phrase_loss)\n",
    "    \n",
    "    return train_loss_avg, train_bar_loss_avg, train_pos_loss_avg, train_token_loss_avg, train_dur_loss_avg, train_phrase_loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ba69f8-9040-4096-ac6b-b829563fa9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(valid_loader, model, epoch, total_epoch):\n",
    "    # define the format of tqdm\n",
    "    with tqdm(total=len(valid_loader), ncols=150) as _tqdm:  \n",
    "        _tqdm.set_description('validation epoch: {}/{}'.format(epoch + 1, total_epoch))  \n",
    "\n",
    "        model.eval()  # switch to valid mode\n",
    "        running_loss = 0.0\n",
    "        val_loss = []\n",
    "        val_bar_loss = []\n",
    "        val_pos_loss = []\n",
    "        val_token_loss = []\n",
    "        val_dur_loss = []\n",
    "        val_phrase_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, data in enumerate((valid_loader)):\n",
    "                try:\n",
    "                    enc_inputs = {k: data[f'src_{k}'].to(device) for k in src_keys}\n",
    "                    dec_inputs = {k: data[f'tgt_{k}'].to(device) for k in tgt_keys}\n",
    "\n",
    "                    bar_out, pos_out, token_out, dur_out, phrase_out = model(enc_inputs, dec_inputs)\n",
    "\n",
    "                    bar_out = bar_out #.logit()\n",
    "                    tgt_bar = (data['tgt_bar'].to(device))[:, 1:]\n",
    "                    bar_loss = xe_loss(bar_out[:, :-1], tgt_bar)\n",
    "\n",
    "                    pos_out = pos_out #.logit()\n",
    "                    tgt_pos = (data['tgt_pos'].to(device))[:, 1:]\n",
    "                    pos_loss = xe_loss(pos_out[:, :-1], tgt_pos)\n",
    "\n",
    "                    token_out = token_out #.logit()\n",
    "                    tgt_token = (data['tgt_token'].to(device))[:, 1:]\n",
    "                    token_loss = xe_loss(token_out[:, :-1], tgt_token)\n",
    "\n",
    "                    dur_out = dur_out #.logit()\n",
    "                    tgt_dur = (data['tgt_dur'].to(device))[:, 1:]\n",
    "                    dur_loss = xe_loss(dur_out[:, :-1], tgt_dur)\n",
    "\n",
    "                    phrase_out = phrase_out# .logit()\n",
    "                    tgt_phrase = (data['tgt_phrase'].to(device))[:, 1:]\n",
    "                    phrase_loss = xe_loss(phrase_out[:, :-1], tgt_phrase)\n",
    "\n",
    "\n",
    "                    # 3) total loss\n",
    "                    total_loss = bar_loss + pos_loss + token_loss + dur_loss + phrase_loss\n",
    "                    val_loss.append(total_loss.item())\n",
    "                    running_loss += total_loss.item()\n",
    "\n",
    "                    val_bar_loss.append(bar_loss.item())\n",
    "                    val_pos_loss.append(pos_loss.item())\n",
    "                    val_token_loss.append(token_loss.item())\n",
    "                    val_dur_loss.append(dur_loss.item())\n",
    "                    val_phrase_loss.append(phrase_loss.item())\n",
    "\n",
    "                    _tqdm.set_postfix(\n",
    "                        loss=\"{:.3f}, bar={:.3f}, pos={:.3f}, token={:.3f}, dur={:.3f}, phrase={:.3f}\".format(total_loss,\n",
    "                                                                                                              bar_loss, \n",
    "                                                                                                              pos_loss,\n",
    "                                                                                                              token_loss,\n",
    "                                                                                                              dur_loss,\n",
    "                                                                                                              phrase_loss))\n",
    "\n",
    "                    _tqdm.update(2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(data)\n",
    "                    print(\"Bad Data Item!\")\n",
    "                    print(e)\n",
    "                    break\n",
    "            \n",
    "    val_loss_avg = np.mean(val_loss)\n",
    "    val_bar_loss_avg = np.mean(val_bar_loss)\n",
    "    val_pos_loss_avg = np.mean(val_pos_loss)\n",
    "    val_token_loss_avg = np.mean(val_token_loss)\n",
    "    val_dur_loss_avg = np.mean(val_dur_loss)\n",
    "    val_phrase_loss_avg = np.mean(val_phrase_loss)\n",
    "\n",
    "    return val_loss_avg, val_bar_loss_avg, val_pos_loss_avg, val_token_loss_avg, val_dur_loss_avg, val_phrase_loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b780a0-a2b8-4ae8-b51d-d8d3bd5d391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_l2m():\n",
    "    ## train melody to lyric generation\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    global device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    # args\n",
    "    set_seed()\n",
    "    # set_hparams()\n",
    "    event2word_dict, word2event_dict = pickle.load(open(f\"{binary_dir}/music_dict.pkl\", 'rb'))\n",
    "\n",
    "    # tensorboard logger\n",
    "    cur_time = get_time()\n",
    "    # tensorboard_dir = hparams['tensorboard']\n",
    "    # train_log_dir = f'{tensorboard_dir}/{cur_time}/train'\n",
    "    # valid_log_dir = f'{tensorboard_dir}/{cur_time}/valid'\n",
    "    # train_writer = SummaryWriter(log_dir=train_log_dir)\n",
    "    # valid_writer = SummaryWriter(log_dir=valid_log_dir)\n",
    "\n",
    "    # ------------\n",
    "    # train\n",
    "    # ------------\n",
    "    # load data\n",
    "    train_dataset = L2MDataset('train', event2word_dict, hparams, shuffle=True)\n",
    "    valid_dataset = L2MDataset('valid', event2word_dict, hparams, shuffle=False)\n",
    "\n",
    "    train_loader = build_dataloader(dataset=train_dataset, shuffle=True, batch_size=hparams['batch_size'], endless=False)\n",
    "    val_loader = build_dataloader(dataset=valid_dataset, shuffle=False, batch_size=hparams['batch_size'], endless=False)\n",
    "    \n",
    "    print(len(train_loader))\n",
    "    \n",
    "    # print(f\"foundation model pth: {hparams['custom_model_dir']}\")\n",
    "    \n",
    "    def tensor_check_fn(key, param, input_param, error_msgs):\n",
    "        if param.shape != input_param.shape:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    model = Bart(event2word_dict=event2word_dict, \n",
    "                 word2event_dict=word2event_dict, \n",
    "                 model_pth='',\n",
    "                 hidden_size=hparams['hidden_size'], \n",
    "                 num_layers=hparams['n_layers'], \n",
    "                 num_heads=hparams['n_head'], \n",
    "                 dropout=hparams['drop_prob'],).to(device)\n",
    "    \n",
    "    pre_trained_path = hparams['pretrain']\n",
    "    if pre_trained_path != '':\n",
    "        current_model_dict = model.state_dict()\n",
    "        loaded_state_dict = torch.load(pre_trained_path)\n",
    "        new_state_dict={k:v if v.size()==current_model_dict[k].size() else current_model_dict[k] for k,v in zip(current_model_dict.keys(), loaded_state_dict.values())}\n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "        print(\">>> Load pretrained model successfully\")\n",
    "        \n",
    "    ## warm up\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=hparams['lr'],\n",
    "        betas=(hparams['optimizer_adam_beta1'], hparams['optimizer_adam_beta2']),\n",
    "        weight_decay=hparams['weight_decay'])\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=hparams['warmup'], num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    # training conditions (for naming the ckpt)\n",
    "    lr = hparams['lr']\n",
    "\n",
    "    # early stop: initialize the early_stopping object\n",
    "    checkpointpath = f\"{hparams['checkpoint_dir']}/checkpoint_{cur_time}_lr_{lr}\"\n",
    "    if not os.path.exists(checkpointpath):\n",
    "        os.mkdir(checkpointpath)\n",
    "    early_stopping = EarlyStopping(patience=hparams['patience'], verbose=True,\n",
    "                                   path=f\"{checkpointpath}/early_stopping_checkpoint.pt\")\n",
    "    \n",
    "\n",
    "    # -------- Train & Validation -------- #\n",
    "    min_valid_running_loss = 1000000  # inf\n",
    "    total_epoch = hparams['total_epoch']\n",
    "    with tqdm(total=total_epoch) as _tqdm:\n",
    "        for epoch in range(total_epoch):\n",
    "            # Train\n",
    "            train_running_loss, _, _, _, _, _ = train(train_loader, model, optimizer, scheduler, epoch, total_epoch)\n",
    "            # train_writer.add_scalars(\"train_epoch_loss\", {\"running\": train_running_loss, 'word': train_word_loss}, epoch)\n",
    "\n",
    "            # validation  \n",
    "            valid_running_loss, _, _, _, _, _ = valid(val_loader, model, epoch, total_epoch)\n",
    "            # valid_writer.add_scalars(\"valid_epoch_loss\", {\"running\": valid_running_loss, 'word': valid_word_loss}, epoch)\n",
    "\n",
    "            # early stopping Check\n",
    "            early_stopping(valid_running_loss, model, epoch)\n",
    "            if early_stopping.early_stop == True:\n",
    "                print(\"Validation Loss convergence， Train over\")\n",
    "                break\n",
    "\n",
    "            # save the best checkpoint\n",
    "            if valid_running_loss < min_valid_running_loss:\n",
    "                min_valid_running_loss = valid_running_loss\n",
    "                torch.save(model.state_dict(), f\"{checkpointpath}/best.pt\")\n",
    "            print(f\"Training Runinng Loss = {train_running_loss}, Validation Running Loss = {min_valid_running_loss}\")  \n",
    "            _tqdm.update(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca57e4-ea82-46a9-93b3-85c50395b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l2m()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1880d-891b-4e10-b289-272a3233c55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xailyr)",
   "language": "python",
   "name": "xailyr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
