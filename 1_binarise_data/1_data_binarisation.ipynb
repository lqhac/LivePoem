{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e707765-742a-4328-a890-85f5bacd0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "root_path = './'  ## replace this with your root path (i.e., path of this current project)\n",
    "os.environ['PYTHONPATH'] = root_path\n",
    "sys.path.append(root_path)\n",
    "import miditoolkit\n",
    "import numpy as np\n",
    "import math\n",
    "import os, pickle, glob, shutil\n",
    "from tqdm import tqdm\n",
    "from utils.indexed_datasets import IndexedDatasetBuilder\n",
    "import multiprocessing as mp\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae1774-fc4e-4dc4-896f-8f9beb5bc553",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = \"\"  ## replace this with a midi test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531ba3e-e10c-46b7-b3f1-9e9240132fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stress_simple (start, dur, reso=480):\n",
    "    ## simplified version of stress detection\n",
    "    ## only considers the first beat of each bar as the strongest beat\n",
    "    bar = 4*reso\n",
    "    unit_len = bar\n",
    "    beat_pos = start - (start//unit_len) * unit_len\n",
    "    beat_num = beat_pos // (unit_len//4)\n",
    "    if beat_num == 0:\n",
    "        return \"<strong>\"\n",
    "    elif beat_num == 2:\n",
    "        return \"<substrong>\"\n",
    "    else:\n",
    "        return \"<weak>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443050b5-ab55-4ba8-8df2-6134cce95219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stress (start, dur, reso=480):\n",
    "    ## full version of stress detection\n",
    "    ## decides the stress based on location and the note duration\n",
    "    if dur in [reso, reso//2, reso//4, reso//8, reso//16, reso*2, reso*4] and start%dur == 0:\n",
    "        ## categorise the note duration\n",
    "        unit_len = 4 * dur\n",
    "    else:\n",
    "        unit_len = 4 * reso\n",
    "    beat_pos = start - (start//unit_len) * unit_len\n",
    "    beat_num = beat_pos // (unit_len//4)\n",
    "    if beat_num == 0:\n",
    "        return \"<strong>\"\n",
    "    elif beat_num == 2:\n",
    "        return \"<substrong>\"\n",
    "    else:\n",
    "        return \"<weak>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5e68e-6eee-43b4-a31d-65dfb77a1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prosody (midi_pth: str):\n",
    "    ## use absolute value\n",
    "    prosody = []\n",
    "    \n",
    "    midi = miditoolkit.MidiFile(midi_pth)\n",
    "    ## group by bar:\n",
    "    bar = {}\n",
    "    ## calculate average note length\n",
    "    note_durs = []\n",
    "    strength, length = [], []\n",
    "    reso = midi.ticks_per_beat\n",
    "    for inst in midi.instruments:\n",
    "        for i, note in enumerate(inst.notes):\n",
    "            strength = stress(start=note.start, dur=note.end-note.start, reso=reso)\n",
    "            length = \"<long>\" if note.end-note.start>reso else \"<short>\"\n",
    "            prosody.append((strength, length))\n",
    "    \n",
    "    return prosody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f710cd-9f29-4666-95a3-3009c3965779",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test cell\n",
    "print(len(prosody(test_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ccd925-baea-4c88-9eb3-fc7ba086e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrasing (midi_pth: str):\n",
    "    ## phrase-level segmentation\n",
    "    ## midi_pth: the path to the midi file to segment \n",
    "    ## (n.b. the midi file has to be monophonic)\n",
    "    midi = miditoolkit.MidiFile(midi_pth)\n",
    "    assert len(midi.instruments) == 1  ## monophonic check (partial)\n",
    "    reso = midi.ticks_per_beat\n",
    "    notes = midi.instruments[0].notes.copy()\n",
    "    \n",
    "    long = []\n",
    "    pause = []\n",
    "    note_info = []\n",
    "    \n",
    "    for idx, note in enumerate(notes):\n",
    "        note_bar = note.start // (4 * reso) ## a bar == 4 beat == 4 * 480 ticks\n",
    "        note_pos = (note.start - (note_bar * 4 * reso)) ## relative position in the current bar\n",
    "        note_pitch = note.pitch\n",
    "        note_dur = note.end - note.start\n",
    "        note_info.append((note_bar, note_pos, note_pitch, note_dur))\n",
    "        if note_dur > reso:\n",
    "            long.append(idx)\n",
    "        if (idx > 0) and (notes[idx].start-notes[idx-1].end >= reso//2):\n",
    "            pause.append(idx-1)\n",
    "    \n",
    "    union = list(set(long + pause))\n",
    "    if 0 in union:\n",
    "        union.remove(0)\n",
    "    if len(notes)-1 in union:\n",
    "        union.remove(len(notes)-1)\n",
    "    union.sort()\n",
    "    \n",
    "    def dur(note: miditoolkit.Note):\n",
    "        return abs(note.end-note.start)\n",
    "    \n",
    "    i = 1\n",
    "    while i<len(union):\n",
    "        if abs(union[i-1]-union[i]) == 1:\n",
    "            if abs(dur(notes[union[i-1]])-dur(notes[union[i]])) > 240:\n",
    "                union.remove(union[i])\n",
    "            else:\n",
    "                union.remove(union[i-1])\n",
    "        i = i + 1\n",
    "    \n",
    "    ## annotate the phrase markers\n",
    "    midi.markers=[]\n",
    "    for k, b in enumerate(union):\n",
    "        midi.markers.append(miditoolkit.Marker(time=notes[b].end, text=f\"Phrase_{k}\"))\n",
    "    \n",
    "    ## uncomment the following line if you intend to save the segmented midi\n",
    "    # midi.dump(os.path.join('./', os.path.basename(midi_pth)[:-4]+'_phrased.mid'))\n",
    "    \n",
    "    is_boundary = []\n",
    "    for i in range(len(notes)):\n",
    "        if i in union:\n",
    "            is_boundary.append(\"<true>\")\n",
    "        else:\n",
    "            is_boundary.append(\"<false>\")\n",
    "    \n",
    "    assert len(note_info) == len(is_boundary)\n",
    "    return is_boundary, note_info, union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1c97e-44d8-4bfc-a6a4-3651fbf2ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes (midi_pth: str):\n",
    "    midi = miditoolkit.MidiFile(midi_pth)\n",
    "    assert len(midi.instruments) == 1  ## monophonic\n",
    "    reso = midi.ticks_per_beat\n",
    "    notes = midi.instruments[0].notes.copy()\n",
    "    \n",
    "    note_info = []\n",
    "    \n",
    "    for note in notes:\n",
    "        note_info.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769712d4-4920-4eab-8717-34d8b9891401",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test cell\n",
    "bound, note_info, _ = phrasing(test_sample)\n",
    "print(len(bound), len(note_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8b17b-fd9a-42ec-80d0-e27d525d9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise (midi_pth, event2word_dict):\n",
    "    ## tokenise a midi sample\n",
    "    ## midi_pth: the path to the midi to tokenise\n",
    "    prsd = prosody(midi_pth)\n",
    "    bound, notes, _ = phrasing(midi_pth)\n",
    "    assert len(prsd) == len(notes)\n",
    "    src_words, tgt_words = [], []\n",
    "    \n",
    "    tgt_words.append({\n",
    "        'bar':event2word_dict['Bar'][f\"<s>\"],\n",
    "        'pos':event2word_dict['Pos'][f\"<pad>\"],\n",
    "        'token':event2word_dict['Pitch'][f\"<pad>\"],\n",
    "        'dur':event2word_dict['Dur'][f\"<pad>\"],\n",
    "        'phrase':event2word_dict['Phrase'][f\"<pad>\"],\n",
    "    })\n",
    "    \n",
    "    for idx in range(len(prsd)):\n",
    "        if notes[idx][0] >= 200:\n",
    "            return [], []\n",
    "        src_words.append({\n",
    "            'strength':event2word_dict['Strength'][prsd[idx][0]],\n",
    "            'length':event2word_dict['Length'][prsd[idx][1]],\n",
    "            'phrase':event2word_dict['Phrase'][bound[idx]],\n",
    "        })\n",
    "        tgt_words.append({\n",
    "            'bar':event2word_dict['Bar'][f\"Bar_{notes[idx][0]}\"],\n",
    "            'pos':event2word_dict['Pos'][f\"Pos_{notes[idx][1]}\"],\n",
    "            'token':event2word_dict['Pitch'][f\"Pitch_{notes[idx][2]}\"],\n",
    "            'dur':event2word_dict['Dur'][f\"Dur_{notes[idx][3]}\"],\n",
    "            'phrase':event2word_dict['Phrase'][bound[idx]],\n",
    "        })\n",
    "    \n",
    "    ## eos\n",
    "    src_words.append({\n",
    "        'strength':event2word_dict['Strength'][f\"</s>\"],\n",
    "        'length':event2word_dict['Length'][f\"</s>\"],\n",
    "        'phrase':event2word_dict['Phrase'][f\"</s>\"],\n",
    "    })\n",
    "    tgt_words.append({\n",
    "        'bar':event2word_dict['Bar'][f\"</s>\"],\n",
    "        'pos':event2word_dict['Pos'][f\"<pad>\"],\n",
    "        'token':event2word_dict['Pitch'][f\"<pad>\"],\n",
    "        'dur':event2word_dict['Dur'][f\"<pad>\"],\n",
    "        'phrase':event2word_dict['Phrase'][f\"<pad>\"],\n",
    "    })\n",
    "    \n",
    "    return src_words, tgt_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011653b9-dd7f-4128-8401-e6d6c1530c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_binary (midi_pth, i, event2word_dict, split):\n",
    "    ## single-file base function of transforming data into binary representations\n",
    "    try:\n",
    "        src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
    "        if len(src_words) == 0 or len(tgt_words) == 0 or len(tgt_words) > 1024:\n",
    "            return None\n",
    "        \n",
    "        data_sample = {\n",
    "            'input_path': midi_pth,\n",
    "            'item_name': os.path.basename(midi_pth),\n",
    "            'src_words': src_words,\n",
    "            'tgt_words': tgt_words,\n",
    "            'word_length': len(tgt_words)\n",
    "        }\n",
    "        \n",
    "        return [data_sample]\n",
    "    \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45826a3e-534b-433e-b034-e38a305e43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2binary(dataset_dirs, words_dir, split, word2event_dict, event2word_dict):\n",
    "    ## batch processing of data binarisation\n",
    "    ## dataset_dirs: directories of data to binarise\n",
    "    ## words_dir: output directory of binarised data\n",
    "    ## split: the name of split of the dataset (e.g., 'train', 'valid', 'test')\n",
    "\n",
    "    # make the output directory if non-existent\n",
    "    save_dir = f'{words_dir}/{split}'\n",
    "    if os.path.exists(save_dir):\n",
    "        shutil.rmtree(save_dir)\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "    midi_files = []\n",
    "    for dataset_dir in dataset_dirs:\n",
    "        midi_files.extend(glob.glob(os.path.join(os.path.join(dataset_dir, split), \"*.mid\")))\n",
    "    \n",
    "    futures = []\n",
    "    ds_builder = IndexedDatasetBuilder(save_dir)  # index dataset\n",
    "    p = mp.Pool(int(os.getenv('N_PROC', 2)))  # 不要开太大，容易内存溢出\n",
    "    \n",
    "    for i in range (len(midi_files)):\n",
    "        futures.append(p.apply_async(data_to_binary, args=[midi_files[i], i, event2word_dict, split]))\n",
    "    p.close()\n",
    "\n",
    "    words_length = []\n",
    "    all_words = []\n",
    "    for f in tqdm(futures):\n",
    "        item = f.get()\n",
    "        if item is None:\n",
    "            continue\n",
    "        for i in range(len(item)):\n",
    "            sample = item[i]\n",
    "            words_length.append(sample['word_length'])\n",
    "            all_words.append(sample)\n",
    "            ds_builder.add_item(sample) # add item index\n",
    "\n",
    "    # save \n",
    "    ds_builder.finalize()\n",
    "    np.save(f'{words_dir}/{split}_words_length.npy', words_length)\n",
    "    np.save(f'{words_dir}/{split}_words.npy', all_words)\n",
    "    p.join()\n",
    "    print(f'| # {split}_tokens: {sum(words_length)}')\n",
    "    \n",
    "    return all_words, words_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7f08d-4645-4223-b8d8-1e4e0b79599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## shuffle and split dataset\n",
    "def split_data(output_dir='./'):\n",
    "    dataset_dirs = [] ## fill in your own directories to datasets\n",
    "    all_files = []\n",
    "    for dataset_dir in dataset_dirs:\n",
    "        all_files.extend(glob.glob(os.path.join(dataset_dir, '*.mid')))\n",
    "    ## shuffle\n",
    "    print(f\"|>>> Total Files: {len(all_files)}\")\n",
    "    \n",
    "    indices = [i for i in range(len(all_files))]\n",
    "    import random, shutil\n",
    "    random.shuffle(indices)\n",
    "    train_end = int(np.floor(0.8*len(all_files)))\n",
    "    valid_end = int(train_end + np.floor(0.1*len(all_files)))\n",
    "    train_idx = indices[:train_end]\n",
    "    valid_idx = indices[train_end:valid_end]\n",
    "    test_idx = indices[valid_end:]\n",
    "    assert len(all_files) == len(train_idx)+len(valid_idx)+len(test_idx)\n",
    "    print(f\"|>>>>> Train Files: {len(train_idx)}\")\n",
    "    print(f\"|>>>>> Valid Files: {len(valid_idx)}\")\n",
    "    print(f\"|>>>>> Test Files: {len(test_idx)}\")\n",
    "    \n",
    "    for split in ['train', 'test', 'valid']:\n",
    "        os.makedirs(os.path.join(output_dir, split), exist_ok=True)\n",
    "    \n",
    "    for t in train_idx:\n",
    "        shutil.copy(all_files[t], os.path.join(f'{output_dir}/train', os.path.basename(all_files[t])))\n",
    "    for v in valid_idx:\n",
    "        shutil.copy(all_files[v], os.path.join(f'{output_dir}/valid', os.path.basename(all_files[v])))\n",
    "    for t in test_idx:\n",
    "        shutil.copy(all_files[t], os.path.join(f'{output_dir}/test', os.path.basename(all_files[t])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c978378d-ecfe-4067-9ab4-66360e678137",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9467fd-c6a7-42c0-b929-45841fe3bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'valid', 'test']:\n",
    "    data2binary(dataset_dirs=dataset_dirs,\n",
    "                words_dir=words_dir,\n",
    "                split=split,\n",
    "                word2event_dict=word2event_dict,\n",
    "                event2word_dict=event2word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378fc830-ccd6-48a9-9372-bc389cf95da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xailyr)",
   "language": "python",
   "name": "xailyr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
